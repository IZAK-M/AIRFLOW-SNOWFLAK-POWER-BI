# Pipeline ETL : Traiterment des donn√©es en temps r√©el

##  üöç Projet ETL GTFS ‚Äì M√©tropole de Nice / Lignes d‚ÄôAzur

Ce projet vise √† construire un pipeline ETL pour exploiter les donn√©es de transport en open data (GTFS / GTFS-RT) fournies par la m√©tropole de Nice et Lignes d‚ÄôAzur. Il permet :

- la collecte et transformation des donn√©es statiques et temps r√©el,

- leur chargement dans Snowflake,

- la pr√©paration de jeux de donn√©es pour l‚Äôanalyse et le suivi (retards, ponctualit√©, √©tat du service).

Des tableaux de bord analytiques illustrent les usages possibles pour la visualisation et l‚Äôaide √† la d√©cision.

## Pipeline ELT (vue d‚Äôensemble)

![ELT](https://github.com/IZAK-M/AIRFLOW-SNOWFLAK-POWER-BI/blob/main/images/ELT.png)

#### Le pipeline suit l‚Äôarchitecture MEDALLION :
- *Bronze* : Ingestion brute des GTFS static et realtime, avec horodatage et source.

- *Silver* : Normalisation des horaires, jointures entre trips/routes/stops, gestion des fuseaux horaires.

- *Gold* : KPIs comme ponctualit√©, fr√©quence, temps d‚Äôattente, par ligne/jour/heure, pr√™ts pour Power BI.

## üõ†Ô∏è Stack technique

 - **Apache Airflow** : orchestration des workflows ETL via des DAGs modulaires et maintenables

 - **Snowflake** : entrep√¥t de donn√©es cloud pour le stockage, la transformation et l‚Äôanalyse des donn√©es

 - **Docker Compose** : environnement local reproductible pour le d√©ploiement d‚ÄôAirflow et de ses services associ√©s

 - **Python** : scripts d‚Äôingestion, parsing des flux Protobuf (GTFS-RT), et transformation des donn√©es au format CSV

## D√©pendances principales

Voir [`requirements.txt`]() :

* `pandas`
* `requests`
* `gtfs-realtime-bindings`
* `snowflake-connector-python`
* `apache-airflow-providers-snowflake==6.5.2`

## License

[MIT](https://choosealicense.com/licenses/mit/)